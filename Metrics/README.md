# Metrics

- *User Stories Satisfaction Coverage -- USSC*: the metric evaluates the degree to which user stories are satisfied, where a user story is considered satisfied if an endpoint or a frontend satisfies its requirement. The metric is computed as $USSC=us_{sod}/us_{tot}\times100$, where $us_{sod}$ is the number of satisfied user stories and $us_{tot}$ is the total number of user stories.

- *Container Integrity Coverage -- CIC*: the metric evaluates whether a container fulfills all the user stories within a specific set (cf. `data_metrics.json`). A container is considered complete if it satisfies every user story of the set it holds. The metric is computed as $CIC=set_{us}/set_{tot}\times100$, where $set_{us}$ is the number of sets of complete user stories and $set_{tot}$ is the total number of sets.

- *Granularity Evaluation -- GE*: the metric evaluates the granularity of the user stories by representing them as nodes (and the system as a graph). The relationship between nodes is established through links, and sets of user stories can reside within the same container. In this context, a clique is defined as a set of nodes where each node has a link to all other nodes within the clique. The $GE$ metric is computed as \
[![](https://latex.codecogs.com/svg.latex?%5C%5C%20%20%20%20%20GE%3D%20%20%5C%5C%20%5Cbegin%7Bcases%7D%20%5C%5C%20%20%20%20%20c_%7Btot%7D%2Fclique_%7Bno%7D%5Ctimes100%20%20%20%20%20%20%20%20%20%20%20%20%26%20%5Ctext%7Bif%20%7D%20c_%7Btot%7D%5Cleq%20clique_%7Bno%7D%5C%5C%20%5C%5C%20%20%20%20%20100%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%26%20%5Ctext%7Bif%20%7D%20clique_%7Bno%7D%3Cc_%7Btot%7D%3C%20set_%7Btot%7D%5C%5C%20%5C%5C%20%20%20%20%20(2%5Ctimes%20set_%7Btot%7D%20-%20c_%7Btot%7D)%2Fset_%7Btot%7D%5Ctimes%20100%20%26%20%5Ctext%7Bif%20%7D%20(2%5Ctimes%20set_%7Btot%7D)%20%3E%20c_%7Btot%7D%20%5Cgeq%20set_%7Btot%7D%5C%5C%20%5C%5C%20%20%20%20%200%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%26%20%5Ctext%7Bif%20%7D%20c_%7Btot%7D%20%5Cgeq%202%5Ctimes%20set_%7Btot%7D%5C%5C%20%5C%5C%20%5Cend%7Bcases%7D%20%5C%5C%20)](#_)\
    where $clique_{no}$ is the number of non-overlapping cliques (computed with the Bron-Kerbosch algorithm), and $c_{tot}$ is the total number of containers.

- *Service Coverage -- SC*: the metric evaluates, at the system level, the overall score of the *Container Service Coverage -- CSC* metric over the containers. $CSC$ evaluates whether a container has at least one backend service for each set of user stories it fully satisfies. If the container provides fewer backend services than the number of cliques, the coverage percentage is proportionally reduced, i.e., $CSC=s_{be}/clique_{full}\times 100$, where $s_{be}$ is the number of backend services, and $clique_{full}$ is the number of fully satisfied cliques. If the number of services exceeds or equals the number of cliques, the coverage is considered complete (i.e., $CSC=100\%$). At system level, $SC$ is computed as $SC=\sum_{i=1}^{n} CSC_{i} \big / c_{tot} \times 100$, where $CSC_i$ is the result of $CSC$ on container $i$. 

- *Persistence Coverage -- PC*: the metric evaluates whether a container has at least one database service for each set of user stories it fully satisfies, given that the set requires database support (i.e., has a `db` field equal to `true` in the `data_metrics.json`). Note that the metric only applies to containers that fulfill sets where a database is required. The metric is computed as $PC=c_{db}/c_{set,db}$, where $c_{db}$ is the number of containers that provide at least one database microservices for the relative sets and $c_{set,db}$ is the number of containers where a database is required.

- *System Container Endpoint Coverage -- SCEC*: the metric evaluates, at the system level, the overall score of the *Container Endpoint Coverage -- CEC* metric over all containers. $CEC$ assesses whether every user story covered by a container has at least one associated endpoint (or web page) within that container to fulfill its requirements. $CEC$ is computed as $CEC=\sum_{i=1}^{n} us_{e_i}/us_{c}\times 100$, where $us_{e_i}$ is the number of user stories covered by an endpoint (or page) within the container and $us_{c}$ is the number of user stories to be covered by a container. At the system level, $SCEC$ is computed as $SCEC=\sum_{i=1}^{n} CEC_{i} / c_{tot} \times 100$, where $CEC_{i}$ is the result of $CEC$ on container $i$. 

- A *Final Index*, averaging the results of the metrics and of the runs, is then computed to provide a final score to ArchiGPT.
